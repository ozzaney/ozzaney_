---
layout: post
title:  "Variational AutoeEncoder is not an AutoEncoder"
categories: [ AI, MachineLearning ]
image: assets/images/vae_main.png
use_math: true
---

# 계속 나오는 VAE! 공부해보자

해당 포스트는 이활석님의 "오토인코더의 모든 것"강의를 정리한 포스트입니다.

## contents

1. revisit DNN
2. manifold learning
3. AutoEncoder
4. Variational AutoEncoder
5. Application

## AutoEncoder

AutoEncoder는 인공신경망 네트워크의 일종으로, 차원축소를 목적으로 데이터의 representation을 학습하기 위한 네트워크이다.
최근에는 AE 모델이 generative model분야에서 자주 사용된다고 한다. (그 이유는 이따가 알아봅시다.)

## 오토인코더의 구조
![AE](https://user-images.githubusercontent.com/85322951/204951758-b1bfb407-8baa-4912-9b40-0130db790376.png)

오토인코더는 다음과 같이 인코더와 디코더가 붙어 있는 구조를 갖고 있다.
오토인코더를 학습할 때는 비지도학습을 따르고 loss는 negative Maximum Likelihood로 해석된다.(뒷부분에서 자세히 설명)
학습완료 후 인코더는 차원축소 모델로 기능하고 디코더는 생성모델로 기능한다.

AutoEncoder에 대한 키워드는 4가지로 볼 수가 있다.

1. Unsupervised learing : 비지도학습, 즉 학습시 라벨링이 필요가 없다!
2. manifold learning: 고차원의 공간 전체에 중요한 정보가 있는게 아니다. 정보가 몰려 있는 manifold 공간을 찾아 차원을 축소시키자.
3. generative model learning: 모델 내 생성파트가 존재한다.
4. Maximum Likelihood density estimation: 우도를 최대화하는 density를 예측한다.

autoencoder를 본격적으로 들어가기 전 먼저 필요한 DNN에 대한 이해를 해보도록 하자.

## revisit DNN

고전적 머신러닝의 방법론을 순서대로 생각해보자.  (비지도학습기준)
1. 먼저 데이터셋을 모은다. (X, y) 
2.  X를 가지고 y를 설명할 좋은 방법은 없을까? 가장 좋은 설명을 하는 model을 정의한다.
 $f_{\theta}()$
모델은 theta의 함수이다.
3. Loss는 target과 예측값의 차이이다. 원하는 Loss를 정의한다. 이때 Loss는 target과 model에 X를 대입한 값에 대한 함수이다.
4. learning 과정은 Loss를 최소화하는 $\theta$를 찾는 과정이다. 이때 closed form으로 argmin L이 주어지면 좋지만 그렇지 않을 경우 gradient descent 방법과 같은 iterative한 방법을 적용한다. 
5. 4번 과정을 통해 얻은 최적의 $\theta$를 f에 대입하여 최종 모델을 얻는다. model에 새로운 x데이터를 대입한 $f_{\theta}$가 $y_{new}$와 가까운지를 본다. (predict)

정리하자면 데이터를 모은 뒤,
이를 잘 설명하는 model을 정의하고 loss ftn을 정의한다.
loss ftn은 X, y, $\theta$의 함수이지만 X, y는 realized value이므로 결국 loss ftn은 $\theta$의 함수이다.
이 loss를 최소화하는  $\theta$를 찾는게 목적이지만 closed form으로 구할 수 없을 경우 GD같은 방법을 이용해 구한다.

deep learning에서 자주 사용되는 loss ftn은 MSE, cross entropy 두가지이다. 다른 loss도 많은데 왜 이 두가지가 자주 사용되는걸까?

이유는 다른 loss도 많은데 왜 이 두가지가 자주 사용되는걸까? 딥러닝처럼 복잡한 모델에 대해서 학습을 할 때는 gradient descent를 구하기 위해서 필요한 gradient가 단순하게 구해지지 않는다.
layer를 통과할 때마다 구해지는 gradient의 곱을 이용해 back propagation 알고리즘을 이용해 weight update를 하게 되는데 back prop을 사용하기 위해서는 loss ftn에 두가지 제약이 따른다.
1. training DB의 전체 데이터에 대한 loss가 sample 데이터에 대한 loss의 합이다.
2. sample별 loss ftn의 입력인자는 label과 neural network의 출력값 두가지여야 한다.

이러한 제약을 만족시키는 mse, cross entropy와 같은 loss ftn이 backprop이 필요한 DNN에 자주 사용되는 것이다.


